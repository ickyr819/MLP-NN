#training
import pandas as pd
from pandas import read_csv
import numpy as np
from numpy import std
from numpy import ravel
from statistics import mean

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import train_test_split
from sklearn.model_selection import learning_curve
from sklearn.model_selection import StratifiedKFold

from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

from tensorflow.python.keras import Sequential
from tensorflow.python.keras.layers import Dense
from tensorflow.python.keras.layers import Dropout
from tensorflow.python.keras import layers
from tensorflow.python.keras import activations 
from keras.layers import BatchNormalization
from keras.callbacks import EarlyStopping
from tensorflow.python.keras.optimizers import adam_v2

from matplotlib import pyplot

columns = ['age','anaemia','creatinine_phosphokinase','diabetes','ejection_fraction','high_blood_pressure','platelets','serum_creatinine','serum_sodium','sex','smoking','time','DEATH_EVENT']

dataframe = read_csv(r"C:\Users\ricky\OneDrive\Desktop\record\record.csv", header=None,sep=",",names=columns)
col = list(dataframe.columns)
#缺失值移除 line N/A.0.NONE
dataframe[col]=dataframe[col].apply(pd.to_numeric,errors='coerce')
dataframe.dropna(how="all",inplace=True)
#轉換資料類型確保資料都是1 or 0
dataframe = pd.DataFrame(dataframe, dtype='float')
#traget
target = dataframe['DEATH_EVENT'].values
type(target)

from collections import Counter
counter = Counter(target)
# count class1/class2 
for k,v in counter.items():
    per = v / len(target) * 100
    print('Class=%s, Count=%s, Percentage=%.3f%%' % (k, v, per))
X, y = dataframe.values[:, :-1], dataframe.values[:, -1]
X = X.astype('float')
#data preprocessing
le=LabelEncoder()
y =le.fit_transform(y)
#normailization
X=preprocessing.scale(X)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

n_features = X.shape[1]
model = Sequential()
#hidden layer 
model.add(Dense(5,kernel_initializer='he_normal', input_shape=(n_features,)))
model.add(BatchNormalization())
model.add(layers.Activation(activations.relu))
#output layer
model.add(Dense(1))
model.add(BatchNormalization())
model.add(layers.Activation(activations.sigmoid))
#optimization
model.compile(optimizer=adam_v2.Adam(learning_rate=0.00042), loss='binary_crossentropy',metrics=["accuracy"])
earlystop = EarlyStopping(monitor='val_loss', patience=10, verbose=0)
#fit model
history = model.fit(X_train, y_train, epochs=200, batch_size=16,verbose=0, validation_data=(X_test,y_test))
yhat = model.predict_classes(X_test)
score = accuracy_score(y_test, yhat)
print('準確率: %.3f' % score)

# plot learning cv
#x epoch y loss
plt.title('Learning Curves')
plt.xlabel('Epoch')
plt.ylabel('Cross Entropy')
plt.plot(history.history['loss'], label='train')
plt.plot(history.history['val_loss'], label='val')
plt.legend()
plt.show()
#x epoch y accuracy
plt.title('Learning Curves')
plt.xlabel('Epoch')
plt.ylabel('accuracy')
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='val')
plt.legend()
plt.show()
